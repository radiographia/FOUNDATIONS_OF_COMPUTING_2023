<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Types of Processors</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part106.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part108.htm">Далее &gt;</a></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark105" name="bookmark925">Types of Processors</a><a name="bookmark969">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">CISC Processors</h4><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark970">One of the earlier goals of CPU designers was to provide more and more instructions in the instruction set of a CPU to ensure that the CPU supports more functions. This makes it easier to translate high-level language programs to machine language and ensures that the machine language programs run more efficiently. Of course, every additional instruction in the instruction set of a CPU requires additional hardware circuitry to handle that instruction, adding more complexity to the CPU&#39;s hardware circuitry. Another goal of CPU designers was to optimize the usage of expensive memory. To achieve this, designers tried to pack more instructions in memory by introducing the concept of variable-length instructions such as half-word, one-and- half-word, etc. For example, an operand in an immediate instruction needs fewer bits, and hence, a CPU designer can design it as a half-word instruction. Additionally, designers originally designed CPUs to support a variety of addressing modes (discussed later in this chapter during the discussion of memory). CPUs with large instruction set, variable-length instructions, and a variety of addressing modes are called CPUs based on </a><i>CISC </i>(<i>Complex Instruction Set Computer</i>) <i>architecture</i>. Since CISC processors possess so many processing features, they make the job of machine language programmers easier. However, they are complex and expensive to produce.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">RISC Processors</h4><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">In early 1980s, some CPU designers realized that many instructions supported by a CISC-based</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">CPU are rarely used. Hence, they came up with the idea of reducing the design complexity of a CPU by implementing in hardware circuitry only bare minimum basic set of instructions and some of the frequently used instructions. The instruction set of the CPU need not support other complex instructions because a computer can implement them in software by using the basic set of instructions. While working on simpler CPU design, the designers also came up with the idea of making all instructions of uniform length so that decoding and execution of all instructions becomes simple and fast. Furthermore, to speed up computation and to reduce the complexity of handling a number of addressing modes, they decided to design all instructions in such a way that they retrieve operands stored in registers in CPU rather than from memory. These design ideas resulted in producing faster and less expensive processors. CPUs with a small instruction set, fixed-length instructions, and reduced references to memory to retrieve operands are called CPUs based on <i>RISC </i>( <i>Reduced Instruction Set Computer</i>) <i>architecture</i>. Since RISC processors have a small instruction set, they place extra demand on programmers who must consider how to implement complex computations by combining simple instructions. However, RISC processors are faster for most applications, less complex, and less expensive to produce than CISC processors because of simpler design.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Popular RISC processors used in workstations are POWER (used in IBM workstations), SPARC (used in SUN workstations), and PA-RISC (used in HP workstations).</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Supporters of RISC technology claim that increased processing speed and lower cost of RISC processors offset the limitations of a reduced instruction set. However, critics of RISC technology are of the opinion that a RISC processor has to process more of these simple programmed instructions to complete a task, placing additional burden on system software. There seems to be no clear answer as to which technology is better. The answer may be that each technology lends itself best to certain applications, and so both technologies will coexist.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">EPIC Processors</h4><p class="s16" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Explicitly Parallel Instruction Computing (EPIC) <span class="p">architecture uses three key techniques for improved performance - explicit parallelism, predication, and speculation. These terminologies are explained here.</span></p><p style="padding-top: 9pt;padding-left: 38pt;text-indent: -15pt;text-align: justify;">1. <b>Explicit parallelism.  </b>EPIC  technology  breaks  through  the  sequential  nature  of conventional processor architectures by allowing the software to communicate explicitly to the processor when the system can perform an operation in parallel. For this, it uses tighter coupling between compiler and processor. It enables the compiler to extract maximum parallelism in original code and explicitly describe it to the processor. At compile time, the compiler detects which of the instructions can the system execute in parallel. It then reorders them and groups them in such a way that the system can execute instructions belonging to separate groups in parallel. At runtime, the processor exploits this explicit parallelism information provided by the compiler to execute the instructions faster.</p><p style="padding-top: 4pt;padding-left: 38pt;text-indent: -15pt;text-align: justify;"><a name="bookmark971">2. </a><b>Predication. </b>Predication technique improves performance by reducing the number of branches and branch mispredicts. The system first takes the help of compiler to reorder the instructions to reduce the number of branches as much as possible at compile time. Conventional processors use “branch prediction” technique in which the processor predicts which way a branch will fork and speculatively executes instructions along the</p><p style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">predicted path. At the time of execution of the branch instruction, if the system finds the prediction to be correct, the processor gains performance improvement because instructions lying in the path to be executed now have already been executed and the processor can use their results directly. However, if the system finds the prediction to be wrong, it discards the results of execution of the predicted path and takes up the instructions of the correct path for execution.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">However, EPIC technology uses “branch predication” instead of “branch prediction”. In this technique, instead of predicting and executing one of the paths of a branch, the processor executes instructions of all the paths of the branch exploiting as much parallelism as possible. Now when the processor discovers the actual branch outcome, it retains the valid results and discards other results. Thus, branch predication effectively removes the negative affect of branch prediction technique in cases of branch mispredict.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">3. <b>Speculation. </b>Speculation technique improves performance by reducing the effect of memory-to-processor speed mismatch. Memory access speed is much slower than processor speed. Speculative data loading technique takes care of this by loading a piece of data and keeping it ready before the processor requires it. It not only allows the processor to load a piece of data from memory before a program needs it, but it also postpones the reporting of exceptions, if the loaded data is illegal.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">We can also implement speculation technique by taking help of compiler. For this, the compiler analyses a program at compile time, looking for any instructions that will need data from memory. It inserts speculative load instructions in the instructions stream of the program well ahead of the instructions that need data from memory. It also inserts a matching speculative check instruction immediately before the instructions that need data from memory. It now reorders the surrounding instructions so that the processor can dispatch them in parallel. Now when the processor encounters a speculative load instruction at runtime, it retrieves and loads the data from memory. When the processor encounters the speculative check instruction, it verifies the load before allowing the program to use the loaded data in the next instruction. If the load is invalid, the processor does not immediately report an exception. It postpones exception reporting until it encounters a check instruction that matches the speculative load. If the load is valid, the system behaves as if the exception never happened.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">Speculation technique combined with predication technique gives the compiler more flexibility to reorder instructions and increase parallelism.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Notice that all three techniques are based on the availability of an intelligent compiler and closed coupling between compiler and processor. Hence, performance of processors based on EPIC technology comes from both hardware and software.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Processors based on EPIC architecture are simpler and more powerful than traditional CISC or RISC processors. These processors are mainly for 64-bit or higher, high-end server and workstation market (not for personal computer market). Intel&#39;s IA-64 (code-named Itanium) was the first EPIC processor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Multicore Processors</h4><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">The approach used earlier for building faster processors was to keep reducing the size of chips,</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">while increasing the number of transistors they contain. Although, this trend continued in computing industry for several decades, it was later realized that transistors cannot shrink forever. Transistor technology limits the ability to continue making single-core processors more powerful due to the following reasons:</p><p style="padding-top: 9pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">1. As a transistor gets smaller, the gate, which switches the electricity ON and OFF, gets thinner and less able to block flow of electrons. Thus, small transistors tend to use electricity all the time, even when they are not swithcing. This wastes power.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">2. Increasing clock speeds causes transistors to switch faster and generate more heat and consume more power.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">These and other challenges forced processor manufacturers to research for new approaches for building faster processors. In response, manufacturers came out with the idea of building multicore processor chips instead of increasingly powerful (faster) single-core processor chips. In this new architecture, a processor chip has multiple cooler-running, more energy-efficient processing cores, instead of one increasingly powerful core. The multicore chips do not necessarily run as fast as the highest performing single-core models, but they improve overall performance by handling more work in parallel. For instance, a dual-core chip running multiple applications is about 1.5 times faster than a chip with just one comparable core.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark972">Operating system (OS) controls overall assignment of tasks in a multicore processor. In a multicore processor, each core has its independent cache (though in some designs all cores share the same cache), thus providing the OS with sufficient resources to handle multiple applications in parallel. When a single-core chip runs multiple programs, the OS assigns a time slice to work on one program and then assigns different time slices for other programs. This can cause conflicts, errors, or slowdowns when the processor must perform multiple tasks simultaneously. However, a multicore chip can run multiple programs at the same time with each core handling a separate program. The same logic holds for running multiple threads of a multithreaded application at the same time on a multicore chip, with each core handling a separate thread. Based on this, either the OS or a multithreaded application parcels out work to multiple cores.</a></p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Multicore processors have the following advantages over single-core processors:</p><p style="padding-top: 9pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">1. They enable building of computers with better overall system performance by handling more work in parallel.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">2. For comparable performance, multicore chips consume less power and generate less heat than single-core chips. Hence, multicore technology is also referred to as <i>energy-efficient </i>or <i>power-aware processor technology</i>.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">3. Because the chips&#39; cores are on the same die in case of multicore processors architecture, they can share architectural components, such as memory elements and memory management. They thus have fewer components and lower costs than systems running multiple chips (each a single-core processor).</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">4. Also, signaling between cores can be faster and use less electricity than on multichip systems.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Multicore processors, however, currently have the following limitations:</p><p style="padding-top: 9pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">1. To take advantage of multicore chips, we must redesign applications so that the processor</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">can run them as multiple threads. Note that it is more challenging to create software that is multithreaded.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">2. To redesign applications, programmers must find good places to break up the applications, divide the work into roughly equal pieces that can run at the same time, and determine the best times for the threads to communicate with one another. All these add to extra work for programmers.</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">3. Software vendors often charge customers for each processor that will run the software (one software license per processor). A customer running an application on an 8-processor machine (multiprocessor computer) with single-core processors would thus pay for 8 licenses. A key issue with multicore chips is whether software vendors should consider a processor to be a single core or an entire chip. Currently, different vendors have different views regarding this issue. Some consider a processor as a unit that plugs into a single socket on the motherboard, regardless of whether it has one or more cores. Hence, a single software license is sufficient for a multicore chip. On the other hand, others charge more to use their software on multicore chips for per-processor licensing. They are of the opinion that customers get added performance benefit by running the software on a chip with multiple cores, so they should pay more. Multicore-chip makers are concerned that this type of non-uniform policy will hurt their products&#39; sales.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Chip makers like Apple, Intel, AMD, IBM, and Sun have already introduced multicore chips for servers, desktops, and laptops. The current multicore chips are dual-core (2 cores per chip), quad-core (4 cores per chip), 8 cores per chip, 16 cores per chip, 32 cores per chip, 64 cores per chip, and so on.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark973">Power-Efficient Processors</a></h4><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Traditionally, a system&#39;s performance was measured in terms of its processing power. However, excessive power consumption by computing systems has now become a matter of concern for most organizations. Cost of operating computing systems often far exceeds the cost of purchasing them. Hence, considering total cost of ownership (TCO), a new measure that has become popular for measuring the performance of computing systems is power consumption to processing power ratio.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Manufacturers of computing systems have therefore made attempts at all levels in computer architecture to reduce power consumption of systems. Processor manufacturers have come out with new processor architectures to reduce power consumption right at processor level. For example, Intel Xeon processor offers a technology called <i>Demand Based Switching (DBS) </i>for reduced power consumption. A traditional processor operates only at a single frequency and voltage, regardless of its workload. Therefore, it is always ON and always consumes full power. Processors based on DBS technology are designed to run at multiple frequency and voltage settings. In conjuction with an Operating System (OS) and Basic Input Output System (BIOS) that support DBS, such processors automatically switch to and operate at the lowest setting that is consistent with optimal application performance. The OS monitors processor utilization multiple times per second and down-shifts/up-shifts to a lower/higher frequency and voltage as appropriate. Therefore, power usage is automatically tailored to match dynamically changing system workload, which substantially reduces power wastage with minimal impact on peak performance capabilities of the processor.</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part106.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part108.htm">Далее &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
