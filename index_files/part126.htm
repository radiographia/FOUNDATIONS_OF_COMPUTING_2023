<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Distributed Memory Systems</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part125.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part127.htm">Далее &gt;</a></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark124" name="bookmark944">Distributed Memory Systems</a><a name="bookmark1018">&zwnj;</a></p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark1019">We saw that shared memory systems can have only a small number of processors due to various constraints imposed by different architectures. Distributed memory systems are more scalable and can have almost unlimited number of processors. Moreover, in contrast to shared memory systems, processors of distributed memory systems can be located far from each other to cover a wider geographical area. These systems consist of multiple computers interconnected together by a communication network. Hence, they are also known as </a><i>multicomputer systems</i><a href="part126.htm#bookmark1020" class="s23">. As </a><a href="part126.htm#bookmark1020" class="s3">Figure </a><span class="s46">4.16</span> shows, each individual computer can itself be a multiprocessor system (a shared memory system). As interconnected computers do not share a common memory (each one has its own local memory), all physical communication between them takes place by passing messages across the communication network that interconnects them. That is, all interactions between processes running on different computers takes place by message passing. Hence, distributed memory systems are also known as <i>message-passing systems</i>. They use exchange of messages to transfer data and work, and to synchronize actions among processes. If any computer of a distributed memory system is a shared-memory multiprocessor system, interactions among processes running on that computer can still take place through its shared memory. For a particular computer, its own resources are<i>local, </i>whereas the other computers and their resources are <i>remote. </i>In a distributed memory system, usually we call a computer and its resources as a <i>node, site, </i>or <i>machine</i>.</p><p class="s46" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="part126.htm#bookmark1020" class="s23">Computers and communication network of </a><a href="part126.htm#bookmark1020" class="s3">Figure </a>4.16<span class="s10"> </span><span class="p">can take different forms. Moreover, roles of different computers and the manner in which they interact with each other may also differ from one distributed memory system to another. Hence, there are many types of distributed memory systems. Two commonly used types are:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><span><img width="330" height="328" alt="image" src="Image_329.jpg"/></span></p><p class="s20" style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;text-align: left;"><a name="bookmark1020">Figure 4.16 </a><span class="s21">. A distributed memory multiprocessor system (also known as multicomputer system).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: -15pt;text-align: justify;">1. <b>Cluster of Workstations (COWs). </b><a href="part126.htm#bookmark1021" class="s23">COWs use off-the-shelf components. Hence, a COW consists of several standard workstations (or personal computers) interconnected with a standard network such as InfiniBand or Gigabit Ethernet. Due to the use of off-the-shelf components (nodes and network), no special measure is taken in hardware architecture to tolerate failure of one or more communication links or nodes. However, COWs often implement fault tolerance features in software to tolerate failure of one or more nodes. Moreover, COWs sometimes use multiple networks to tolerate network failures. For example, a cluster may simultaneously have both Myrinet and Gigabit Ethernet so that if one of the networks fails, the cluster can still function with the other one. Multiple networks are also useful for different applications performance because of their varying bandwidth and latency requirements. Hence, some applications may perform better on one network than on the other network. Usually, a system based on COW neatly mounts its nodes and network(s) in one or more racks (see </a><a href="part126.htm#bookmark1021" class="s3">Figure </a><span class="s46">4.17</span>).</p><p style="padding-top: 4pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">2. <b>Blade servers. </b>There has been a rapid growth during the last few years in usage of information technology in all types of organizations. Due to this, organizations are experiencing tremendous growth in the information that they have to handle and are looking for better options to manage their data. Organizations are facing the following challenges in scaling of traditional data centers, which are built of multiple rack-mounted servers:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 131pt;text-indent: 0pt;text-align: left;"><span><img width="334" height="243" alt="image" src="Image_330.jpg"/></span></p><p class="s20" style="padding-top: 8pt;padding-left: 35pt;text-indent: 0pt;text-align: center;"><a name="bookmark1021">Figure 4.17. </a><span class="s21">Off-the-shelf components (nodes and network(s)) of a small Cluster of Workstations (COW) neatly mounted in a rack. Larger clusters use higher racks to accommodate more number of nodes in a single rack. Very large clusters use many racks kept side-by-side to accommodate large number of nodes.</span></p><p style="padding-top: 8pt;padding-left: 69pt;text-indent: -15pt;text-align: left;">a. Power consumption and associated cooling requirements of a data centre grows fast as it scales up.</p><p style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">b. Traditional servers also need more floor space.</p><p style="padding-top: 4pt;padding-left: 54pt;text-indent: 0pt;text-align: left;">c. They require complex cabling, which in turn weakens system reliability.</p><p style="padding-top: 4pt;padding-left: 69pt;text-indent: -15pt;text-align: left;">d. They are more difficult to manage and require expensive, skilled support resources, leading to high total cost of ownership (TCO).</p><p style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">Computer vendors designed blade servers to address these challenges. Blade servers are multi-processor systems, which offer a standardized method of deploying multiple processors, memory, and I/O resources by placing them on plug-in boards that slide into a standard chassis. They reduce cost as compared to separate server systems by sharing common resources like power, storage, and I/O across multiple processors.</p><p class="s46" style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;"><a href="part126.htm#bookmark1023" class="s23">As </a><a href="part126.htm#bookmark1023" class="s3">Figure </a>4.18<span class="s10"> </span><span class="p">shows, blade servers consist of a standardized chassis in which we can insert server blades. Each server blade is a separate system that runs its own operating system and application software independently of the rest of the system, relying on its local processor(s), memory, and I/O resources to handle its applications. The chassis contains shared resources (power, cooling, storage, I/O ports, network connections, etc.) that all server blades in the chassis share. A server blade slides into the chassis and connects to the shared resources through a backplane or midplane connection. It is hot swappable, allowing live upgrades to the server resource pool. Hence, we can add processing resources to a data centre by sliding in another server blade in the chassis without interrupting the functioning of other server blades. A typical blade server chassis can hold 8 to16 physical server blades, each with 2 to 8 processor cores. With virtualization, we can create several virtual machines on each server blade, yielding a compact system with hundreds of virtual servers for a data centre. Hence, the key hardware and software components of a blade server are:</span></p><p style="padding-top: 9pt;padding-left: 54pt;text-indent: 0pt;text-align: left;">a. Standardized chassis</p><p style="padding-top: 3pt;padding-left: 69pt;text-indent: -15pt;text-align: left;">b. Server blades containing multi-core processors, memory, local storage, and chassis interface</p><p style="padding-top: 4pt;padding-left: 54pt;text-indent: 0pt;text-align: left;">c. Shared storage, I/O ports, network connections, etc.</p><p style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">d. Shared power and cooling</p><p style="padding-top: 4pt;padding-left: 54pt;text-indent: 0pt;text-align: left;">e. Virtualization software</p><p style="padding-top: 3pt;padding-left: 55pt;text-indent: 0pt;text-align: left;"><a name="bookmark1022">f. Software for management of resources of individual server blades</a></p><p style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: left;">g. Software for power and cooling monitoring and management</p><p style="padding-top: 9pt;padding-left: 39pt;text-indent: 0pt;text-align: justify;">As compared to multiple rack-mounted servers, blade servers offer the following advantages to data centers:</p><p style="padding-top: 8pt;padding-left: 69pt;text-indent: -15pt;text-align: justify;">a. They provide dramatic reduction in floor space, power consumption, and cooling requirement because of integration of servers, storage, networking, I/O devices, and other components in a single, modular chassis. Due to this, we often refer to a blade server as a “data centre in a box”.</p><p style="padding-top: 4pt;padding-left: 69pt;text-indent: -15pt;text-align: justify;">b. They help eliminate complex cabling, thereby improving overall data centre reliability.</p><p style="padding-top: 4pt;padding-left: 69pt;text-indent: -15pt;text-align: justify;">c. They provide simplified management while retaining enterprise class performance, reliability, and availability.</p><p style="padding-top: 3pt;padding-left: 69pt;text-indent: -15pt;text-align: justify;">d. They handle scalability with greater ease. We can add new functionality by sliding another server blade in the chassis.</p><p style="padding-top: 4pt;padding-left: 69pt;text-indent: -15pt;text-align: justify;">e. They reduce total cost of ownership (TCO) of a data centre by enabling sharing of expensive resources, dramatically improving resource utilization, and reducing</p><p style="padding-top: 3pt;padding-left: 69pt;text-indent: 0pt;text-align: left;">management and operation cost.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 79pt;text-indent: 0pt;text-align: left;"><span><img width="473" height="276" alt="image" src="Image_331.jpg"/></span></p><p class="s20" style="padding-top: 8pt;padding-left: 41pt;text-indent: 0pt;text-align: center;"><a name="bookmark1023">Figure 4.18. </a><span class="s21">A blade server.</span></p><p style="padding-top: 9pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">In-spite-of all its advantages, users still have the following concerns with blade servers:</p><p style="padding-top: 8pt;padding-left: 68pt;text-indent: -15pt;text-align: left;">a. Blade servers do not perform well in case of large-scale transaction processing applications.</p><p style="padding-top: 4pt;padding-left: 68pt;text-indent: -15pt;text-align: left;">b. Chassis systems are proprietary, leading to vendor lock-in problem. We need to purchase all server blades from the same vendor, which supplied the chassis.</p><p style="padding-top: 3pt;padding-left: 68pt;text-indent: -15pt;text-align: left;">c. Blade servers are not economical for applications requiring less than 5 to 10 servers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part125.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part127.htm">Далее &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
