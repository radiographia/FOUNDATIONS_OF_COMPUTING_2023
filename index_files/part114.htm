<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Cache Memory</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part113.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part115.htm">Далее &gt;</a></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark112" name="bookmark932">Cache Memory</a><a name="bookmark988">&zwnj;</a></p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Cache memory is an extremely fast, small memory between the CPU and main memory whose access time is closer to the processing speed of CPU. It acts as a high-speed buffer between CPU and main memory and computer systems use it to temporarily store active data and instructions during processing. Since cache memory is faster than main memory, system&#39;s performance improves considerably when the system makes those data and instructions available in cache that processes need during present processing.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark989">The obvious question is how the system knows in advance, which data and instructions it needs in present processing to make it available beforehand in the cache. The answer to this question comes from the </a><i>principle of locality of reference</i>. According to this principle, during the course of execution of programs, memory references by the processor for both instructions and data tend to cluster. That is, if the system executes an instruction, there is a likelihood of the system executing the same instruction again soon. This is true because typically most programs contain a number of iterative loops (such as <i>while </i>and <i>for </i>loops). Once a program enters a loop during its execution, there are repeated references to a small set of instructions (i.e., the instructions in the loop are executed repeatedly several times). A branch instruction (such as a <i>go to </i>statement) upsets the locality of reference to instructions, but such instructions do not occur frequently in well-written programs. Locality of reference is true not only for references to program instructions, but also for references to data. For example, operations on tables, arrays, and records in a file involve access to a clustered set of data words. Over a long period, the instructions, or data clusters in use change, but over a short period, the processor works primarily with fixed clusters of instructions and data resulting in clustered memory references.</p><p class="s46" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="part114.htm#bookmark990" class="s23">Based on the feature of locality of reference, a system makes effective use of a cache in the following manner. As </a><a href="part114.htm#bookmark990" class="s3">Figure </a>4.7<span class="s10"> </span><span class="p">shows, cache memory acts as a small, fast-speed buffer between the CPU and main memory. It contains a copy of a portion of main memory contents. When the CPU attempts to read a memory word (instruction or data) during execution of a program, the system checks whether the word is in the cache. If so, the system delivers the word</span></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">to the CPU from the cache. If not, the system reads a block of main memory, consisting of some fixed number of words including the requested word, into the cache and then delivers the requested word to the CPU. Because of the feature of locality of reference, when the system fetches a block of memory words into the cache to satisfy a single memory reference, it is likely that there will soon be references to other words in that block. That is, the next time the CPU attempts to read a word, it is likely that it finds it in the cache and saves the time needed to read the word from main memory. You might think that the odds of the CPU finding the word it needs in the cache are small, but statistics shows that in more than 90% of time, the needed word is available in the cache.</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">As the name implies, cache memory is a memory in hiding (the word “cache” literally means a hiding place for treasure or stores) and is not addressable by normal users of the computer system. The hardware transfers the needed instructions/data between the cache and main memory without any programmer intervention. In fact, application programmers are unaware of its presence and use.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 146pt;text-indent: 0pt;text-align: left;"><span><img width="250" height="200" alt="image" src="Image_315.jpg"/></span></p><p class="s20" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: center;"><a name="bookmark990">Figure 4.7. </a><span class="s21">Illustrating the operation of cache memory.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="part114.htm#bookmark990" class="s3" name="bookmark991">Figure </a><span class="s46">4.7</span> illustrates the general concept of cache memory usage. Actual systems may have some variations. For example, many computer systems have two separate cache memories called <i>instruction cache </i>and <i>data cache</i>. They use instruction cache for storing program instructions and data cache for storing data. This allows faster identification of availability of accessed word in cache memory and helps in further improving system&#39;s performance. Many computer systems also have multiple levels of caches (such as level one and level two caches, often referred to as L1 and L2 caches). L1 cache is smaller than L2 cache and the system uses it to store more frequently accessed instructions/data as compared to those in L2 cache.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Designers need to address several design issues to use cache memory. A detailed discussion of cache design is beyond the scope of this book. Key design issues are summarized here.</p><p style="padding-top: 9pt;padding-left: 39pt;text-indent: -15pt;text-align: justify;">1. <b>Cache size. </b>Cache memory is expensive as compared to main memory and hence its size is normally small. Statistical studies indicate that reasonably small caches can have a significant impact on overall system performance. Hence, designers usually decide cache memory size on the basis of statistical studies of program behavior, and choose cache size in such a way that the probability of the system finding the needed instruction/data in cache is more than 90%. As a typical example of cache size, a system having 1 GB of main memory may have about 1 MB of cache memory.</p><p style="padding-top: 3pt;padding-left: 38pt;text-indent: -15pt;text-align: justify;">2. <b>Block size</b>. Block size refers to the unit of data (few memory words) exchanged between cache and main memory. As we increase block size, hit ratio (fraction of time that referenced instruction/data is found in cache) first increases because of the principle of locality since more and more useful words are brought into the cache. However, hit ratio begins to decrease if we increase block size further because the probability of using the newly fetched words becomes less than the probability of reusing the words that the system must move out of the cache to make room for the new block. Hence, the designer must suitably choose block size to maximize hit ratio.</p><p style="padding-top: 4pt;padding-left: 38pt;text-indent: -15pt;text-align: justify;">3. <b>Replacement policy. </b>When the system has to fetch a new block into the cache, it may have to replace another one to make room for the new block. The replacement policy decides which block to replace in such situation. Obviously, it will be best to replace a block that the system is least likely to need again in the near future. Although, it is impossible to identify such a block, a reasonably effective strategy is to replace the block that has been in the cache longest with no reference to it. This policy is known as least recently used (LRU) algorithm. The system needs hardware mechanisms to identify the least recently used block.</p><p style="padding-top: 4pt;padding-left: 38pt;text-indent: -15pt;text-align: justify;">4. <b>Write policy. </b>If the system alters the contents of a block, then it is necessary to write it back to main memory before replacing it. The write policy decides when the system writes back the altered words of a block to main memory. At one extreme, the system writes an updated word of a block to main memory as soon as such updates occur in the block. At the other extreme, the system writes all updated words of the block to main memory only when it replaces the block from the cache. The latter policy minimizes memory write operations but temporarily leaves main memory in an inconsistent (obsolete) state. This can interfere with multiple-processor operation when multiple processors share a common memory and cache. It can also interfere with direct memory access by input/output modules. Several other write policies that are somewhere in between these two extremes are also in use. Normally, the designer of a computer&#39;s architecture makes the choice of a suitable write policy for its cache.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part113.htm">&lt; Назад</a><span> | </span><a href="../index.html">Содержимое</a><span> | </span><a href="part115.htm">Далее &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
